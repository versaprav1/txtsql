my goal

planner agent -this should know how to do different queries first to know the name of the schema, then also it should know the tables and also the data. as for larger db i want tht once it finds the schema it keeps memory for that run and it only updates if the information is new and latest. It needs to save information with the db name, schema name in this way it can go seach this info when it after first query knows which db and which schema then it can use saved info. also based on the cjhanges it should update its data and also keep a log of the changes. 


Planner Agent Design
1. Progressive Database Discovery
The Planner Agent should follow a hierarchical approach to database exploration:

First, identify available database schemas
Then, discover tables within those schemas
Finally, understand the data structure (columns, relationships) within those tables

2. Memory and Caching System
Persistent Memory: The agent should maintain a memory of database structures it has discovered
Hierarchical Storage: Information should be organized by:
Database name
Schema name
Table names and structures
Column details (types, constraints, etc.)
Session Persistence: This information should persist throughout a user session

3. Intelligent Caching Strategy
Only query for schema/table information once per session
Cache the results for future queries
Only refresh the cache when:
New information is needed
The structure might have changed
Explicitly requested by the user

4. Change Tracking
Maintain a log of database structure changes detected
Record when tables or schemas are added, modified, or removed
Track when the agent refreshes its knowledge of the database
5. Efficient Query Planning
Use cached knowledge to construct more efficient queries
Avoid redundant schema exploration queries
Leverage known relationships between tables
6. Implementation Approach
The enhanced Planner Agent would need:

Memory Store: A structured dictionary or object to store database metadata:
db_metadata = {
    "database_name": {
        "schemas": {
            "schema_name": {
                "tables": {
                    "table_name": {
                        "columns": [
                            {"name": "column_name", "type": "data_type", "constraints": [...]}
                        ],
                        "relationships": [...],
                        "last_updated": timestamp
                    }
                },
                "last_updated": timestamp
            }
        },
        "last_updated": timestamp
    }
}

Change Log:

change_log = [
    {
        "timestamp": datetime,
        "action": "discovered_schema/discovered_table/updated_table",
        "details": {...}
    }
]
Progressive Discovery Functions:
discover_schemas(): Queries information_schema to find all schemas
discover_tables(schema): Finds all tables in a given schema
discover_table_structure(schema, table): Gets column details for a specific table
Caching Logic:
Check if metadata exists before querying the database
Implement a time-based or event-based cache invalidation strategy
Provide methods to force-refresh the cache when needed
This enhanced Planner Agent would significantly improve performance for repeated queries against the same database by reducing redundant metadata queries. It would also provide better context for query generation since it would have a more complete understanding of the database structure.




Detailed Implementation Steps for Enhanced Planner Agent
Here's a step-by-step implementation plan for enhancing the Planner Agent with database discovery, caching, and change tracking capabilities:

1. Create Database Metadata Store
First, let's create a class to manage the database metadata:

# database_metadata.py
import json
import os
from datetime import datetime
from typing import Dict, List, Optional, Any

class DatabaseMetadataStore:
    def __init__(self, cache_dir: str = ".cache"):
        self.metadata = {}
        self.change_log = []
        self.cache_dir = cache_dir
        self._ensure_cache_dir()
        
    def _ensure_cache_dir(self):
        """Ensure the cache directory exists"""
        if not os.path.exists(self.cache_dir):
            os.makedirs(self.cache_dir)
    
    def get_cache_path(self, db_name: str) -> str:
        """Get the path to the cache file for a database"""
        return os.path.join(self.cache_dir, f"{db_name}_metadata.json")
    
    def get_log_path(self, db_name: str) -> str:
        """Get the path to the change log file for a database"""
        return os.path.join(self.cache_dir, f"{db_name}_changelog.json")
    
    def load_metadata(self, db_name: str) -> bool:
        """Load metadata from cache if it exists"""
        cache_path = self.get_cache_path(db_name)
        if os.path.exists(cache_path):
            try:
                with open(cache_path, 'r') as f:
                    self.metadata[db_name] = json.load(f)
                return True
            except Exception as e:
                print(f"Error loading metadata cache: {e}")
        return False
    
    def save_metadata(self, db_name: str) -> bool:
        """Save metadata to cache"""
        if db_name not in self.metadata:
            return False
        
        cache_path = self.get_cache_path(db_name)
        try:
            with open(cache_path, 'w') as f:
                json.dump(self.metadata[db_name], f, indent=2)
            return True
        except Exception as e:
            print(f"Error saving metadata cache: {e}")
            return False
    
    def load_change_log(self, db_name: str) -> bool:
        """Load change log from file if it exists"""
        log_path = self.get_log_path(db_name)
        if os.path.exists(log_path):
            try:
                with open(log_path, 'r') as f:
                    self.change_log = json.load(f)
                return True
            except Exception as e:
                print(f"Error loading change log: {e}")
        return False
    
    def save_change_log(self, db_name: str) -> bool:
        """Save change log to file"""
        log_path = self.get_log_path(db_name)
        try:
            with open(log_path, 'w') as f:
                json.dump(self.change_log, f, indent=2)
            return True
        except Exception as e:
            print(f"Error saving change log: {e}")
            return False
    
    def log_change(self, db_name: str, action: str, details: Dict[str, Any]) -> None:
        """Add an entry to the change log"""
        self.change_log.append({
            "timestamp": datetime.now().isoformat(),
            "database": db_name,
            "action": action,
            "details": details
        })
        self.save_change_log(db_name)
    
    def get_schemas(self, db_name: str) -> List[str]:
        """Get list of schemas for a database"""
        if db_name in self.metadata and "schemas" in self.metadata[db_name]:
            return list(self.metadata[db_name]["schemas"].keys())
        return []
    
    def get_tables(self, db_name: str, schema: str) -> List[str]:
        """Get list of tables for a schema"""
        if (db_name in self.metadata and 
            "schemas" in self.metadata[db_name] and 
            schema in self.metadata[db_name]["schemas"] and
            "tables" in self.metadata[db_name]["schemas"][schema]):
            return list(self.metadata[db_name]["schemas"][schema]["tables"].keys())
        return []
    
    def get_table_structure(self, db_name: str, schema: str, table: str) -> Dict[str, Any]:
        """Get structure of a table"""
        if (db_name in self.metadata and 
            "schemas" in self.metadata[db_name] and 
            schema in self.metadata[db_name]["schemas"] and
            "tables" in self.metadata[db_name]["schemas"][schema] and
            table in self.metadata[db_name]["schemas"][schema]["tables"]):
            return self.metadata[db_name]["schemas"][schema]["tables"][table]
        return {}
    
    def update_schemas(self, db_name: str, schemas: List[str]) -> None:
        """Update the list of schemas for a database"""
        if db_name not in self.metadata:
            self.metadata[db_name] = {"schemas": {}, "last_updated": datetime.now().isoformat()}
        
        # Track new schemas
        existing_schemas = set(self.get_schemas(db_name))
        new_schemas = set(schemas) - existing_schemas
        
        # Initialize new schemas
        for schema in new_schemas:
            if schema not in self.metadata[db_name]["schemas"]:
                self.metadata[db_name]["schemas"][schema] = {
                    "tables": {},
                    "last_updated": datetime.now().isoformat()
                }
                self.log_change(db_name, "discovered_schema", {"schema": schema})
        
        self.metadata[db_name]["last_updated"] = datetime.now().isoformat()
        self.save_metadata(db_name)
    
    def update_tables(self, db_name: str, schema: str, tables: List[str]) -> None:
        """Update the list of tables for a schema"""
        if db_name not in self.metadata:
            self.metadata[db_name] = {"schemas": {}, "last_updated": datetime.now().isoformat()}
        
        if schema not in self.metadata[db_name]["schemas"]:
            self.metadata[db_name]["schemas"][schema] = {
                "tables": {},
                "last_updated": datetime.now().isoformat()
            }
        
        # Track new tables
        existing_tables = set(self.get_tables(db_name, schema))
        new_tables = set(tables) - existing_tables
        
        # Initialize new tables
        for table in new_tables:
            if table not in self.metadata[db_name]["schemas"][schema]["tables"]:
                self.metadata[db_name]["schemas"][schema]["tables"][table] = {
                    "columns": [],
                    "relationships": [],
                    "last_updated": datetime.now().isoformat()
                }
                self.log_change(db_name, "discovered_table", {"schema": schema, "table": table})
        
        self.metadata[db_name]["schemas"][schema]["last_updated"] = datetime.now().isoformat()
        self.metadata[db_name]["last_updated"] = datetime.now().isoformat()
        self.save_metadata(db_name)
    
    def update_table_structure(self, db_name: str, schema: str, table: str, 
                              columns: List[Dict[str, Any]], 
                              relationships: Optional[List[Dict[str, Any]]] = None) -> None:
        """Update the structure of a table"""
        if db_name not in self.metadata:
            self.metadata[db_name] = {"schemas": {}, "last_updated": datetime.now().isoformat()}
        
        if schema not in self.metadata[db_name]["schemas"]:
            self.metadata[db_name]["schemas"][schema] = {
                "tables": {},
                "last_updated": datetime.now().isoformat()
            }
        
        if table not in self.metadata[db_name]["schemas"][schema]["tables"]:
            self.metadata[db_name]["schemas"][schema]["tables"][table] = {
                "columns": [],
                "relationships": [],
                "last_updated": datetime.now().isoformat()
            }
        
        # Check if structure has changed
        current_columns = self.metadata[db_name]["schemas"][schema]["tables"][table]["columns"]
        if current_columns != columns:
            self.metadata[db_name]["schemas"][schema]["tables"][table]["columns"] = columns
            self.log_change(db_name, "updated_table_structure", {
                "schema": schema, 
                "table": table,
                "columns_changed": True
            })
        
        if relationships is not None:
            current_relationships = self.metadata[db_name]["schemas"][schema]["tables"][table]["relationships"]
            if current_relationships != relationships:
                self.metadata[db_name]["schemas"][schema]["tables"][table]["relationships"] = relationships
                self.log_change(db_name, "updated_table_relationships", {
                    "schema": schema, 
                    "table": table
                })
        
        self.metadata[db_name]["schemas"][schema]["tables"][table]["last_updated"] = datetime.now().isoformat()
        self.metadata[db_name]["schemas"][schema]["last_updated"] = datetime.now().isoformat()
        self.metadata[db_name]["last_updated"] = datetime.now().isoformat()
        self.save_metadata(db_name)
    
    def is_metadata_fresh(self, db_name: str, max_age_hours: int = 24) -> bool:
        """Check if metadata is fresh (updated within max_age_hours)"""
        if db_name not in self.metadata or "last_updated" not in self.metadata[db_name]:
            return False
        
        last_updated = datetime.fromisoformat(self.metadata[db_name]["last_updated"])
        age = datetime.now() - last_updated
        return age.total_seconds() < max_age_hours * 3600
    
    def is_schema_fresh(self, db_name: str, schema: str, max_age_hours: int = 24) -> bool:
        """Check if schema metadata is fresh"""
        if (db_name not in self.metadata or 
            "schemas" not in self.metadata[db_name] or
            schema not in self.metadata[db_name]["schemas"] or
            "last_updated" not in self.metadata[db_name]["schemas"][schema]):
            return False
        
        last_updated = datetime.fromisoformat(self.metadata[db_name]["schemas"][schema]["last_updated"])
        age = datetime.now() - last_updated
        return age.total_seconds() < max_age_hours * 3600
    
    def is_table_fresh(self, db_name: str, schema: str, table: str, max_age_hours: int = 24) -> bool:
        """Check if table metadata is fresh"""
        if (db_name not in self.metadata or 
            "schemas" not in self.metadata[db_name] or
            schema not in self.metadata[db_name]["schemas"] or
            "tables" not in self.metadata[db_name]["schemas"][schema] or
            table not in self.metadata[db_name]["schemas"][schema]["tables"] or
            "last_updated" not in self.metadata[db_name]["schemas"][schema]["tables"][table]):
            return False
        
        last_updated = datetime.fromisoformat(
            self.metadata[db_name]["schemas"][schema]["tables"][table]["last_updated"]
        )
        age = datetime.now() - last_updated
        return age.total_seconds() < max_age_hours * 3600



2. Create Database Discovery Service
Next, let's create a service to discover database structures:

# database_discovery.py
import psycopg2
from typing import Dict, List, Any, Optional, Tuple
from database_metadata import DatabaseMetadataStore

class DatabaseDiscoveryService:
    def __init__(self, metadata_store: DatabaseMetadataStore):
        self.metadata_store = metadata_store
    
    def get_connection(self, db_name: str, user: str, password: str, 
                      host: str, port: str) -> Tuple[Any, Any]:
        """Get a database connection and cursor"""
        conn = psycopg2.connect(
            dbname=db_name,
            user=user,
            password=password,
            host=host,
            port=port,
            connect_timeout=5
        )
        cursor = conn.cursor()
        return conn, cursor
    
    def discover_schemas(self, db_name: str, user: str, password: str, 
                        host: str, port: str, force_refresh: bool = False) -> List[str]:
        """Discover all schemas in the database"""
        # Check if we have fresh metadata
        if not force_refresh and self.metadata_store.is_metadata_fresh(db_name):
            return self.metadata_store.get_schemas(db_name)
        
        # Connect to the database
        try:
            conn, cursor = self.get_connection(db_name, user, password, host, port)
            
            # Query for schemas
            cursor.execute("""
                SELECT schema_name 
                FROM information_schema.schemata 
                WHERE schema_name NOT IN ('information_schema', 'pg_catalog', 'pg_toast')
                ORDER BY schema_name;
            """)
            
            schemas = [row[0] for row in cursor.fetchall()]
            
            # Update metadata
            self.metadata_store.update_schemas(db_name, schemas)
            
            # Clean up
            cursor.close()
            conn.close()
            
            return schemas
        except Exception as e:
            print(f"Error discovering schemas: {e}")
            return []
    
    def discover_tables(self, db_name: str, schema: str, user: str, password: str, 
                       host: str, port: str, force_refresh: bool = False) -> List[str]:
        """Discover all tables in a schema"""
        # Check if we have fresh metadata
        if not force_refresh and self.metadata_store.is_schema_fresh(db_name, schema):
            return self.metadata_store.get_tables(db_name, schema)
        
        # Connect to the database
        try:
            conn, cursor = self.get_connection(db_name, user, password, host, port)
            
            # Query for tables
            cursor.execute("""
                SELECT table_name 
                FROM information_schema.tables 
                WHERE table_schema = %s
                AND table_type = 'BASE TABLE'
                ORDER BY table_name;
            """, (schema,))
            
            tables = [row[0] for row in cursor.fetchall()]
            
            # Update metadata
            self.metadata_store.update_tables(db_name, schema, tables)
            
            # Clean up
            cursor.close()
            conn.close()
            
            return tables
        except Exception as e:
            print(f"Error discovering tables in schema {schema}: {e}")
            return []
    
    def discover_table_structure(self, db_name: str, schema: str, table: str, 
                               user: str, password: str, host: str, port: str,
                               force_refresh: bool = False) -> Dict[str, Any]:
        """Discover the structure of a table"""
        # Check if we have fresh metadata
        if not force_refresh and self.metadata_store.is_table_fresh(db_name, schema, table):
            return self.metadata_store.get_table_structure(db_name, schema, table)
        
        # Connect to the database
        try:
            conn, cursor = self.get_connection(db_name, user, password, host, port)
            
            # Query for columns
            cursor.execute("""
                SELECT column_name, data_type, is_nullable, column_default
                FROM information_schema.columns
                WHERE table_schema = %s AND table_name = %s
                ORDER BY ordinal_position;
            """, (schema, table))
            
            columns = [
                {
                    "name": row[0],
                    "type": row[1],
                    "nullable": row[2] == "YES",
                    "default": row[3]
                }
                for row in cursor.fetchall()
            ]
            
            # Query for primary key
            cursor.execute("""
                SELECT kcu.column_name
                FROM information_schema.table_constraints tc
                JOIN information_schema.key_column_usage kcu
                    ON tc.constraint_name = kcu.constraint_name
                    AND tc.table_schema = kcu.table_schema
                WHERE tc.constraint_type = 'PRIMARY KEY'
                    AND tc.table_schema = %s
                    AND tc.table_name = %s
                ORDER BY kcu.ordinal_position;
            """, (schema, table))
            
            pk_columns = [row[0] for row in cursor.fetchall()]
            
            # Mark primary key columns
            for column in columns:
                if column["name"] in pk_columns:
                    column["primary_key"] = True
            
            # Query for foreign keys
            cursor.execute("""
                SELECT
                    kcu.column_name,
                    ccu.table_schema AS foreign_table_schema,
                    ccu.table_name AS foreign_table_name,
                    ccu.column_name AS foreign_column_name
                FROM information_schema.table_constraints AS tc
                JOIN information_schema.key_column_usage AS kcu
                    ON tc.constraint_name = kcu.constraint_name
                    AND tc.table_schema = kcu.table_schema
                JOIN information_schema.constraint_column_usage AS ccu
                    ON ccu.constraint_name = tc.constraint_name
                    AND ccu.table_schema = tc.table_schema
                WHERE tc.constraint_type = 'FOREIGN KEY'
                    AND tc.table_schema = %s
                    AND tc.table_name = %s;
            """, (schema, table))
            
            relationships = [
                {
                    "column": row[0],
                    "references": {
                        "schema": row[1],
                        "table": row[2],
                        "column": row[3]
                    }
                }
                for row in cursor.fetchall()
            ]
            
            # Update metadata
            self.metadata_store.update_table_structure(
                db_name, schema, table, columns, relationships
            )
            
            # Clean up
            cursor.close()
            conn.close()
            
            return self.metadata_store.get_table_structure(db_name, schema, table)
        except Exception as e:
            print(f"Error discovering structure of table {schema}.{table}: {e}")
            return {}
    
    def get_sample_data(self, db_name: str, schema: str, table: str, 
                      user: str, password: str, host: str, port: str,
                      limit: int = 10) -> Dict[str, Any]:
        """Get sample data from a table"""
        try:
            conn, cursor = self.get_connection(db_name, user, password, host, port)
            
            # Query for sample data
            cursor.execute(f"""
                SELECT * FROM "{schema}"."{table}" LIMIT %s;
            """, (limit,))
            
            columns = [desc[0] for desc in cursor.description]
            rows = cursor.fetchall()
            
            # Clean up
            cursor.close()
            conn.close()
            
            return {
                "columns": columns,
                "rows": rows,
                "row_count": len(rows)
            }
        except Exception as e:
            print(f"Error getting sample data from {schema}.{table}: {e}")
            return {
                "columns": [],
                "rows": [],
                "row_count": 0,
                "error": str(e)
            }



3. Enhance the Planner Agent
Now, let's enhance the Planner Agent to use our new services:

# enhanced_planner_agent.py
from typing import Dict, List, Any, Optional
from database_metadata import DatabaseMetadataStore
from database_discovery import DatabaseDiscoveryService

class EnhancedPlannerAgent:
    def __init__(self, cache_dir: str = ".cache"):
        self.metadata_store = DatabaseMetadataStore(cache_dir)
        self.discovery_service = DatabaseDiscoveryService(self.metadata_store


Update the Planner Prompt Template
Modify the  planner_prompt_template to include instructions about database structure discovery:



planner_prompt_template = """
You are the Planner Agent responsible for analyzing user questions and determining the best approach to answer them.

Your task is to:
1. Identify the type of query (SQL, metadata, etc.)
2. Determine the primary table or data source needed
3. Identify relevant columns or fields
4. Specify any filtering conditions
5. Provide processing instructions

Database Structure Discovery:
- For queries about database structure (schemas, tables, columns), I will help discover this information
- For queries about specific tables, I will provide table structure details
- I maintain a cache of database metadata to avoid redundant queries

When analyzing a question, consider:
- Is this a request for database structure information?
- Which schema and tables are relevant?
- What columns are needed to answer the question?
- Are there any joins or relationships to consider?
- What filtering or aggregation is required?

Return your analysis as a structured JSON object with the following fields:
- query_type: The type of query (sql, metadata, etc.)
- primary_table_or_datasource: The main table or data source
- relevant_columns: List of columns needed
- filtering_conditions: Any WHERE clauses or filters
- processing_instructions: Additional instructions for downstream agents

Example:
For "Show me all orders from customers in New York":
{
  "query_type": "sql",
  "primary_table_or_datasource": "orders JOIN customers",
  "relevant_columns": ["orders.id", "orders.date", "customers.name"],
  "filtering_conditions": "customers.state = 'NY'",
  "processing_instructions": "Join orders with customers table and filter by state"
}
"""


streamlit_app/
├── database/
│   ├── __init__.py
│   ├── metadata_store.py
│   └── discovery_service.py


Flow Execution Path:
The workflow starts at the planner node
Goes to selector node
Then to SQLGenerator node (which detects "list all tables" and generates a standard query)
Then to reviewer node
Then to sql_executor node (which executes the query against the database)
Then to router node
Then directly to final_report node
And finally to end node